{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fa4a3e-01c2-4c36-bd10-ee16741af510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "Shape of X: (45593, 24)\n",
      "Target: pay_received\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/Users/prajitbaskaran/Downloads/delivery_with_synthetic_pay.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Drop non-explainable columns\n",
    "# -----------------------------\n",
    "drop_cols = [\n",
    "    \"Order_ID\",\n",
    "    \"Delivery_person_ID\",\n",
    "    \"Restaurant_latitude\",\n",
    "    \"Restaurant_longitude\",\n",
    "    \"Delivery_location_latitude\",\n",
    "    \"Delivery_location_longitude\",\n",
    "    \"order_time\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Handle categorical columns\n",
    "# -----------------------------\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Handle missing values\n",
    "# -----------------------------\n",
    "# Fill numeric columns with median\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Separate features and target\n",
    "# -----------------------------\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Target:\", TARGET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77aa5a6-2bff-47cb-bd78-69a3bf070675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing complete.\n",
      "ðŸ“ Saved to: /Users/prajitbaskaran/Downloads/Preprocessed.csv\n",
      "Final dataset shape: (45593, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "input_path = \"/Users/prajitbaskaran/Downloads/delivery_with_synthetic_pay.csv\"\n",
    "output_path = \"/Users/prajitbaskaran/Downloads/Preprocessed.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Drop non-explainable columns\n",
    "# -----------------------------\n",
    "drop_cols = [\n",
    "    \"Order_ID\",\n",
    "    \"Delivery_person_ID\",\n",
    "    \"Restaurant_latitude\",\n",
    "    \"Restaurant_longitude\",\n",
    "    \"Delivery_location_latitude\",\n",
    "    \"Delivery_location_longitude\",\n",
    "    \"order_time\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Encode categorical columns\n",
    "# -----------------------------\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Handle missing values\n",
    "# -----------------------------\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Save preprocessed dataset\n",
    "# -----------------------------\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"âœ… Preprocessing complete.\")\n",
    "print(\"ðŸ“ Saved to:\", output_path)\n",
    "print(\"Final dataset shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2304270-28be-4745-bc62-64d7c5c37a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FINAL preprocessing complete\n",
      "ðŸ“ Saved to: /Users/prajitbaskaran/Downloads/2delivery_with_synthetic_pay.csv\n",
      "Final shape: (45593, 22)\n",
      "\n",
      "Age sanity check:\n",
      "count    45593.000000\n",
      "mean        29.584739\n",
      "std          5.696333\n",
      "min         15.000000\n",
      "25%         25.000000\n",
      "50%         30.000000\n",
      "75%         34.000000\n",
      "max         50.000000\n",
      "Name: Delivery_person_Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# =============================\n",
    "# 1. Load ORIGINAL dataset\n",
    "# =============================\n",
    "input_path = \"/Users/prajitbaskaran/Downloads/delivery_with_synthetic_pay.csv\"\n",
    "output_path = \"/Users/prajitbaskaran/Downloads/2delivery_with_synthetic_pay.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# =============================\n",
    "# 2. Drop non-interpretable / ID columns\n",
    "# =============================\n",
    "drop_cols = [\n",
    "    \"ID\",\n",
    "    \"Order_ID\",\n",
    "    \"Delivery_person_ID\",\n",
    "    \"Restaurant_latitude\",\n",
    "    \"Restaurant_longitude\",\n",
    "    \"Delivery_location_latitude\",\n",
    "    \"Delivery_location_longitude\",\n",
    "    \"order_time\",\n",
    "    \"Order_Date\",\n",
    "    \"Time_Ordered\",\n",
    "    \"Time_Order_picked\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# =============================\n",
    "# 3. Ensure correct numeric types\n",
    "# =============================\n",
    "numeric_cols_force = [\n",
    "    \"Delivery_person_Age\",\n",
    "    \"Delivery_person_Ratings\",\n",
    "    \"distance_km\",\n",
    "    \"hour\",\n",
    "    \"Time_taken(min)\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols_force:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# =============================\n",
    "# 4. Convert boolean to int\n",
    "# =============================\n",
    "if \"is_peak_hour\" in df.columns:\n",
    "    df[\"is_peak_hour\"] = df[\"is_peak_hour\"].astype(int)\n",
    "\n",
    "# =============================\n",
    "# 5. Explicit categorical encoding\n",
    "# =============================\n",
    "categorical_cols = [\n",
    "    \"City\",\n",
    "    \"Weatherconditions\",\n",
    "    \"Road_traffic_density\",\n",
    "    \"Type_of_order\",\n",
    "    \"Type_of_vehicle\",\n",
    "    \"Vehicle_condition\",\n",
    "    \"Festival\"\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# =============================\n",
    "# 6. Handle missing values\n",
    "# =============================\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# =============================\n",
    "# 7. Save preprocessed dataset\n",
    "# =============================\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"âœ… FINAL preprocessing complete\")\n",
    "print(\"ðŸ“ Saved to:\", output_path)\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"\\nAge sanity check:\")\n",
    "print(df[\"Delivery_person_Age\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341ed77-8663-4cef-b6cc-bb2544e969d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/Users/prajitbaskaran/Downloads/delivery_with_synthetic_pay.csv\"\n",
    "output_path = \"/Users/prajitbaskaran/Downloads/2delivery_with_synthetic_pay.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9143e4d0-e95f-4391-aaab-28c60248ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FINAL preprocessing complete\n",
      "ðŸ“ Saved to: /Users/prajitbaskaran/Downloads/3delivery_with_synthetic_pay.csv\n",
      "Final shape: (45593, 21)\n",
      "\n",
      "ðŸ”Ž Sanity checks:\n",
      "Age range:\n",
      "count    45593.000000\n",
      "mean        29.584739\n",
      "std          5.696333\n",
      "min         15.000000\n",
      "25%         25.000000\n",
      "50%         30.000000\n",
      "75%         34.000000\n",
      "max         50.000000\n",
      "Name: Delivery_person_Age, dtype: float64\n",
      "\n",
      "Hour distribution:\n",
      "hour\n",
      "19.0    6326\n",
      "21.0    4686\n",
      "22.0    4576\n",
      "20.0    4539\n",
      "23.0    4511\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pay stats:\n",
      "count    45593.000000\n",
      "mean       148.479375\n",
      "std         61.600237\n",
      "min         43.630000\n",
      "25%         94.940000\n",
      "50%        142.830000\n",
      "75%        191.090000\n",
      "max        302.650000\n",
      "Name: pay_received, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# =============================\n",
    "# 1. Load ORIGINAL dataset\n",
    "# =============================\n",
    "input_path = \"/Users/prajitbaskaran/Downloads/delivery_with_synthetic_pay.csv\"\n",
    "output_path = \"/Users/prajitbaskaran/Downloads/3delivery_with_synthetic_pay.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# =============================\n",
    "# 2. FIX TIME PROPERLY\n",
    "# =============================\n",
    "# Convert Time_Orderd to datetime and extract hour\n",
    "if \"Time_Orderd\" in df.columns:\n",
    "    df[\"Time_Orderd\"] = pd.to_datetime(df[\"Time_Orderd\"], format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "    df[\"hour\"] = df[\"Time_Orderd\"].dt.hour\n",
    "\n",
    "# =============================\n",
    "# 3. Drop non-interpretable / ID / raw time columns\n",
    "# =============================\n",
    "drop_cols = [\n",
    "    \"ID\",\n",
    "    \"Order_ID\",\n",
    "    \"Delivery_person_ID\",\n",
    "    \"Restaurant_latitude\",\n",
    "    \"Restaurant_longitude\",\n",
    "    \"Delivery_location_latitude\",\n",
    "    \"Delivery_location_longitude\",\n",
    "    \"order_time\",\n",
    "    \"Order_Date\",\n",
    "    \"Time_Ordered\",\n",
    "    \"Time_Order_picked\",\n",
    "    \"Time_Orderd\"   # raw time dropped AFTER hour extraction\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# =============================\n",
    "# 4. FORCE correct numeric types\n",
    "# =============================\n",
    "numeric_cols_force = [\n",
    "    \"Delivery_person_Age\",\n",
    "    \"Delivery_person_Ratings\",\n",
    "    \"distance_km\",\n",
    "    \"hour\",\n",
    "    \"Time_taken(min)\",\n",
    "    \"surge_bonus\",\n",
    "    \"effort_bonus\",\n",
    "    \"rating_penalty\",\n",
    "    \"policy_multiplier\",\n",
    "    \"pay_received\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols_force:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# =============================\n",
    "# 5. Convert boolean flags\n",
    "# =============================\n",
    "if \"is_peak_hour\" in df.columns:\n",
    "    df[\"is_peak_hour\"] = df[\"is_peak_hour\"].astype(int)\n",
    "\n",
    "# =============================\n",
    "# 6. Encode ONLY true categorical columns\n",
    "# =============================\n",
    "categorical_cols = [\n",
    "    \"City\",\n",
    "    \"Weatherconditions\",\n",
    "    \"Road_traffic_density\",\n",
    "    \"Type_of_order\",\n",
    "    \"Type_of_vehicle\",\n",
    "    \"Vehicle_condition\",\n",
    "    \"Festival\"\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# =============================\n",
    "# 7. Handle missing values\n",
    "# =============================\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# =============================\n",
    "# 8. CAP EXTREME PAY OUTLIERS (CRITICAL FIX)\n",
    "# =============================\n",
    "if \"pay_received\" in df.columns:\n",
    "    upper_cap = df[\"pay_received\"].quantile(0.99)\n",
    "    df[\"pay_received\"] = df[\"pay_received\"].clip(upper=upper_cap)\n",
    "\n",
    "# =============================\n",
    "# 9. Save CLEAN dataset\n",
    "# =============================\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"âœ… FINAL preprocessing complete\")\n",
    "print(\"ðŸ“ Saved to:\", output_path)\n",
    "print(\"Final shape:\", df.shape)\n",
    "\n",
    "print(\"\\nðŸ”Ž Sanity checks:\")\n",
    "print(\"Age range:\")\n",
    "print(df[\"Delivery_person_Age\"].describe())\n",
    "\n",
    "print(\"\\nHour distribution:\")\n",
    "print(df[\"hour\"].value_counts().head())\n",
    "\n",
    "print(\"\\nPay stats:\")\n",
    "print(df[\"pay_received\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69aa6af0-4bed-420e-8314-67e22635aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     86\u001b[39m flip_idx = np.random.choice(\n\u001b[32m     87\u001b[39m     N_FEATURES,\n\u001b[32m     88\u001b[39m     size=np.random.randint(\u001b[32m1\u001b[39m, MAX_FLIPS + \u001b[32m1\u001b[39m),\n\u001b[32m     89\u001b[39m     replace=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     90\u001b[39m )\n\u001b[32m     91\u001b[39m candidate[flip_idx] = \u001b[32m1\u001b[39m - candidate[flip_idx]\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m reward = \u001b[43mevaluate_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reward > best_reward:\n\u001b[32m     96\u001b[39m     best_reward = reward\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mevaluate_subset\u001b[39m\u001b[34m(mask)\u001b[39m\n\u001b[32m     49\u001b[39m X_tr, X_te, y_tr, y_te = train_test_split(\n\u001b[32m     50\u001b[39m     X_sel, y_sub, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m model = GradientBoostingRegressor(\n\u001b[32m     54\u001b[39m     n_estimators=\u001b[32m80\u001b[39m,\n\u001b[32m     55\u001b[39m     max_depth=\u001b[32m3\u001b[39m,\n\u001b[32m     56\u001b[39m     learning_rate=\u001b[32m0.1\u001b[39m,\n\u001b[32m     57\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m preds = model.predict(X_te)\n\u001b[32m     62\u001b[39m rmse = np.sqrt(mean_squared_error(y_te, preds))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:666\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_state()\n\u001b[32m    662\u001b[39m \u001b[38;5;66;03m# Check input\u001b[39;00m\n\u001b[32m    663\u001b[39m \u001b[38;5;66;03m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m sample_weight_is_none = sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    675\u001b[39m sample_weight = _check_sample_weight(sample_weight, X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2917\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2918\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     out = X, y\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1314\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1309\u001b[39m         estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1333\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1074\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1069\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1070\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1071\u001b[39m     )\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1083\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:133\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:182\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    168\u001b[39m     msg_err += (\n\u001b[32m    169\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nGradientBoostingRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# SHAP-GUIDED RL FEATURE SELECTION\n",
    "# ONE-CELL PIPELINE\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------- CONFIG --------\n",
    "INPUT_PATH = \"/Users/prajitbaskaran/Downloads/3delivery_with_synthetic_pay.csv\"\n",
    "OUTPUT_PATH = \"FeatureSelected_FINAL.csv\"\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "N_EPISODES = 30        # RL iterations (keep small for speed)\n",
    "MAX_FLIPS = 2          # PPO-style constrained actions\n",
    "SUBSAMPLE_FRAC = 0.25  # speed-up\n",
    "\n",
    "# -------- LOAD DATA --------\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "X_full = df.drop(columns=[TARGET])\n",
    "y_full = df[TARGET]\n",
    "\n",
    "feature_names = X_full.columns.tolist()\n",
    "N_FEATURES = len(feature_names)\n",
    "\n",
    "# Subsample for RL evaluation\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X_full, y_full, train_size=SUBSAMPLE_FRAC, random_state=42\n",
    ")\n",
    "\n",
    "# -------- ENVIRONMENT --------\n",
    "def evaluate_subset(mask):\n",
    "    selected = [f for f, m in zip(feature_names, mask) if m == 1]\n",
    "\n",
    "    if len(selected) < 4:\n",
    "        return -1e6\n",
    "\n",
    "    X_sel = X_sub[selected]\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_sel, y_sub, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=80,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, preds))\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X_tr, check_additivity=False)\n",
    "\n",
    "    shap_var = np.mean(np.var(np.abs(shap_vals), axis=0))\n",
    "\n",
    "    # Reward: accuracy + SHAP stability + sparsity\n",
    "    reward = (\n",
    "        - rmse\n",
    "        - 0.4 * shap_var\n",
    "        - 0.03 * len(selected)\n",
    "    )\n",
    "\n",
    "    return reward\n",
    "\n",
    "# -------- PPO-INSPIRED RL LOOP --------\n",
    "current_mask = np.random.randint(0, 2, N_FEATURES)\n",
    "best_mask = current_mask.copy()\n",
    "best_reward = -np.inf\n",
    "\n",
    "for ep in range(N_EPISODES):\n",
    "\n",
    "    candidate = current_mask.copy()\n",
    "    flip_idx = np.random.choice(\n",
    "        N_FEATURES,\n",
    "        size=np.random.randint(1, MAX_FLIPS + 1),\n",
    "        replace=False\n",
    "    )\n",
    "    candidate[flip_idx] = 1 - candidate[flip_idx]\n",
    "\n",
    "    reward = evaluate_subset(candidate)\n",
    "\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_mask = candidate.copy()\n",
    "        current_mask = candidate.copy()\n",
    "\n",
    "    print(f\"Episode {ep:02d} | Reward={reward:.4f} | Features={candidate.sum()}\")\n",
    "\n",
    "# -------- FINAL FEATURE SET --------\n",
    "selected_features = [\n",
    "    f for f, m in zip(feature_names, best_mask) if m == 1\n",
    "]\n",
    "\n",
    "print(\"\\nâœ… FINAL SELECTED FEATURES:\")\n",
    "for f in selected_features:\n",
    "    print(\"-\", f)\n",
    "\n",
    "# -------- SAVE FINAL DATASET --------\n",
    "final_df = df[selected_features + [TARGET]]\n",
    "final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\nðŸ“ Saved feature-selected dataset to:\", OUTPUT_PATH)\n",
    "print(\"Final shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c441a1a2-afcf-40ed-98fb-46be70915c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PREPROCESSING COMPLETE\n",
      "Saved to: /Users/prajitbaskaran/Downloads/finalig_v01.csv\n",
      "Final shape: (45555, 14)\n",
      "\n",
      "Sanity checks:\n",
      "Age range: 20.0 - 50.0\n",
      "Hour unique: [0.0, 8.0, 9.0, 10.0, 11.0] ...\n",
      "Pay stats:\n",
      " count    45555.000000\n",
      "mean       148.450890\n",
      "std         61.568836\n",
      "min         43.630000\n",
      "25%         94.940000\n",
      "50%        142.830000\n",
      "75%        191.090000\n",
      "max        302.640000\n",
      "Name: pay_received, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "INPUT_PATH = \"/Users/prajitbaskaran/Downloads/delivery_with_synthetic_pay.csv\"\n",
    "OUTPUT_PATH = \"/Users/prajitbaskaran/Downloads/finalig_v01.csv\"\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# ===============================\n",
    "# 1. DROP UNWANTED / DANGEROUS COLUMNS\n",
    "# ===============================\n",
    "drop_cols = [\n",
    "    # IDs\n",
    "    \"ID\", \"Order_ID\", \"Delivery_person_ID\",\n",
    "\n",
    "    # Exact locations (not explainable)\n",
    "    \"Restaurant_latitude\", \"Restaurant_longitude\",\n",
    "    \"Delivery_location_latitude\", \"Delivery_location_longitude\",\n",
    "\n",
    "    # Raw timestamps (will derive hour instead)\n",
    "    \"order_time\", \"Order_Date\",\n",
    "    \"Time_Ordered\", \"Time_Order_picked\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# ===============================\n",
    "# 2. HANDLE TIME PROPERLY\n",
    "# ===============================\n",
    "# Extract hour from Time_Orderd if present\n",
    "if \"Time_Orderd\" in df.columns:\n",
    "    df[\"Time_Orderd\"] = pd.to_datetime(\n",
    "        df[\"Time_Orderd\"], format=\"%H:%M:%S\", errors=\"coerce\"\n",
    "    )\n",
    "    df[\"hour\"] = df[\"Time_Orderd\"].dt.hour\n",
    "    df = df.drop(columns=[\"Time_Orderd\"])\n",
    "\n",
    "# ===============================\n",
    "# 3. FIX DATA TYPES (CRITICAL)\n",
    "# ===============================\n",
    "numeric_cols = [\n",
    "    \"Delivery_person_Age\",\n",
    "    \"Delivery_person_Ratings\",\n",
    "    \"distance_km\",\n",
    "    \"hour\",\n",
    "    \"Time_taken(min)\",\n",
    "    \"surge_bonus\",\n",
    "    \"effort_bonus\",\n",
    "    \"rating_penalty\",\n",
    "    \"policy_multiplier\",\n",
    "    TARGET\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ===============================\n",
    "# 4. CONVERT BOOLEAN FLAGS\n",
    "# ===============================\n",
    "if \"is_peak_hour\" in df.columns:\n",
    "    df[\"is_peak_hour\"] = df[\"is_peak_hour\"].astype(int)\n",
    "\n",
    "# ===============================\n",
    "# 5. HANDLE MISSING VALUES\n",
    "# ===============================\n",
    "# Numeric â†’ median\n",
    "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Categorical â†’ mode\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# ===============================\n",
    "# 6. REMOVE IMPOSSIBLE VALUES\n",
    "# ===============================\n",
    "# Age sanity\n",
    "if \"Delivery_person_Age\" in df.columns:\n",
    "    df = df[(df[\"Delivery_person_Age\"] >= 18) & (df[\"Delivery_person_Age\"] <= 60)]\n",
    "\n",
    "# Hour sanity\n",
    "if \"hour\" in df.columns:\n",
    "    df = df[(df[\"hour\"] >= 0) & (df[\"hour\"] <= 23)]\n",
    "\n",
    "# ===============================\n",
    "# 7. CAP EXTREME PAY OUTLIERS\n",
    "# ===============================\n",
    "upper_cap = df[TARGET].quantile(0.99)\n",
    "df[TARGET] = df[TARGET].clip(upper=upper_cap)\n",
    "\n",
    "# ===============================\n",
    "# 8. FINAL SAFETY CHECK\n",
    "# ===============================\n",
    "# Remove any remaining non-numeric columns EXCEPT target\n",
    "non_numeric = df.drop(columns=[TARGET]).select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "df = df.drop(columns=non_numeric)\n",
    "\n",
    "# ===============================\n",
    "# 9. SAVE CLEAN DATASET\n",
    "# ===============================\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"âœ… PREPROCESSING COMPLETE\")\n",
    "print(\"Saved to:\", OUTPUT_PATH)\n",
    "print(\"Final shape:\", df.shape)\n",
    "\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Age range:\", df[\"Delivery_person_Age\"].min(), \"-\", df[\"Delivery_person_Age\"].max())\n",
    "print(\"Hour unique:\", sorted(df[\"hour\"].unique())[:5], \"...\")\n",
    "print(\"Pay stats:\\n\", df[TARGET].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de7323-c6fb-4405-b818-c86d2cff5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8425a258-0233-432d-aac2-b7bfb768cb9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['pay_received'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     96\u001b[39m df = df.drop(columns=empty_cols)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# 9. FINAL SAFETY: KEEP ONLY NUMERIC FEATURES\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m non_numeric = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTARGET\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m    102\u001b[39m                 .select_dtypes(exclude=[\u001b[33m\"\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m\"\u001b[39m]).columns\n\u001b[32m    103\u001b[39m df = df.drop(columns=non_numeric)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# 10. SAVE CLEAN DATASET\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:5603\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mdrop\u001b[39m(\n\u001b[32m   5456\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5457\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5464\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5465\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5467\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5601\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5609\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5610\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4810\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4810\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4813\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4852\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4850\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4851\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4852\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4853\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4855\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['pay_received'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "INPUT_PATH = \"/Users/prajitbaskaran/Downloads/last.csv\"\n",
    "OUTPUT_PATH = \"/Users/prajitbaskaran/Downloads/last_preprocessed.csv\"\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# ===============================\n",
    "# 1. DROP UNWANTED / UNSAFE COLUMNS\n",
    "# ===============================\n",
    "drop_cols = [\n",
    "    # IDs\n",
    "    \"ID\", \"Order_ID\", \"Delivery_person_ID\",\n",
    "\n",
    "    # Exact locations (not explainable)\n",
    "    \"Restaurant_latitude\", \"Restaurant_longitude\",\n",
    "    \"Delivery_location_latitude\", \"Delivery_location_longitude\",\n",
    "\n",
    "    # Raw timestamps\n",
    "    \"order_time\", \"Order_Date\",\n",
    "    \"Time_Ordered\", \"Time_Order_picked\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# ===============================\n",
    "# 2. HANDLE TIME (EXTRACT HOUR)\n",
    "# ===============================\n",
    "if \"Time_Orderd\" in df.columns:\n",
    "    df[\"Time_Orderd\"] = pd.to_datetime(\n",
    "        df[\"Time_Orderd\"], format=\"%H:%M:%S\", errors=\"coerce\"\n",
    "    )\n",
    "    df[\"hour\"] = df[\"Time_Orderd\"].dt.hour\n",
    "    df = df.drop(columns=[\"Time_Orderd\"])\n",
    "\n",
    "# ===============================\n",
    "# 3. FORCE NUMERIC TYPES\n",
    "# ===============================\n",
    "numeric_cols = [\n",
    "    \"Delivery_person_Age\",\n",
    "    \"Delivery_person_Ratings\",\n",
    "    \"distance_km\",\n",
    "    \"hour\",\n",
    "    \"Time_taken(min)\",\n",
    "    \"surge_bonus\",\n",
    "    \"effort_bonus\",\n",
    "    \"rating_penalty\",\n",
    "    \"policy_multiplier\",\n",
    "    TARGET\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ===============================\n",
    "# 4. BOOLEAN â†’ INT\n",
    "# ===============================\n",
    "if \"is_peak_hour\" in df.columns:\n",
    "    df[\"is_peak_hour\"] = df[\"is_peak_hour\"].astype(int)\n",
    "\n",
    "# ===============================\n",
    "# 5. HANDLE MISSING VALUES\n",
    "# ===============================\n",
    "# Numeric â†’ median\n",
    "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# ===============================\n",
    "# 6. REMOVE IMPOSSIBLE VALUES\n",
    "# ===============================\n",
    "if \"Delivery_person_Age\" in df.columns:\n",
    "    df = df[(df[\"Delivery_person_Age\"] >= 18) & (df[\"Delivery_person_Age\"] <= 60)]\n",
    "\n",
    "if \"hour\" in df.columns:\n",
    "    df = df[(df[\"hour\"] >= 0) & (df[\"hour\"] <= 23)]\n",
    "\n",
    "# ===============================\n",
    "# 7. CAP EXTREME PAY OUTLIERS\n",
    "# ===============================\n",
    "upper_cap = df[TARGET].quantile(0.99)\n",
    "df[TARGET] = df[TARGET].clip(upper=upper_cap)\n",
    "\n",
    "# ===============================\n",
    "# 8. DROP EMPTY (ALL-NaN) COLUMNS  â† CRITICAL FIX\n",
    "# ===============================\n",
    "empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "df = df.drop(columns=empty_cols)\n",
    "\n",
    "# ===============================\n",
    "# 9. FINAL SAFETY: KEEP ONLY NUMERIC FEATURES\n",
    "# ===============================\n",
    "non_numeric = df.drop(columns=[TARGET]) \\\n",
    "                .select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "df = df.drop(columns=non_numeric)\n",
    "\n",
    "# ===============================\n",
    "# 10. SAVE CLEAN DATASET\n",
    "# ===============================\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"âœ… FINAL PREPROCESSING COMPLETE\")\n",
    "print(\"Saved to:\", OUTPUT_PATH)\n",
    "print(\"Final shape:\", df.shape)\n",
    "\n",
    "print(\"\\nDropped empty columns:\", empty_cols)\n",
    "print(\"\\nSanity checks:\")\n",
    "print(\"Age range:\", df[\"Delivery_person_Age\"].min(), \"-\", df[\"Delivery_person_Age\"].max())\n",
    "print(\"Hour unique sample:\", sorted(df[\"hour\"].unique())[:5])\n",
    "print(\"Pay stats:\\n\", df[TARGET].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eed02ce8-1108-4eff-9c8a-143cd1079b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random Forest RÂ² score: 0.9923\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATASET\n",
    "# ===============================\n",
    "df = pd.read_csv(\"/Users/prajitbaskaran/Downloads/1finalig_v02.csv\")\n",
    "\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "# ===============================\n",
    "# TRAIN-TEST SPLIT\n",
    "# ===============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# TRAIN RANDOM FOREST (SIMPLE)\n",
    "# ===============================\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# EVALUATE\n",
    "# ===============================\n",
    "y_pred = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"âœ… Random Forest RÂ² score:\", round(r2, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9f0866-4d99-4bb2-82c4-f02bd5cc3ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Noise added to pay_received\n",
      "ðŸ“ Saved to: /Users/prajitbaskaran/Downloads/1finalig_v02.csv\n",
      "\n",
      "New pay statistics:\n",
      "count    45555.000000\n",
      "mean       148.452278\n",
      "std         61.778520\n",
      "min         28.307871\n",
      "25%         97.193862\n",
      "50%        142.799249\n",
      "75%        189.802780\n",
      "max        315.440423\n",
      "Name: pay_received, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "input_path = \"/Users/prajitbaskaran/Downloads/finalig_v02.csv\"\n",
    "output_path = \"/Users/prajitbaskaran/Downloads/1finalig_v02.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# ===============================\n",
    "# ADD SMALL GAUSSIAN NOISE\n",
    "# ===============================\n",
    "np.random.seed(42)  # reproducibility\n",
    "\n",
    "noise = np.random.normal(loc=0, scale=5, size=len(df))\n",
    "df[\"pay_received\"] = df[\"pay_received\"] + noise\n",
    "\n",
    "# Ensure pay is non-negative\n",
    "df[\"pay_received\"] = df[\"pay_received\"].clip(lower=0)\n",
    "\n",
    "# ===============================\n",
    "# SAVE NEW DATASET\n",
    "# ===============================\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"âœ… Noise added to pay_received\")\n",
    "print(\"ðŸ“ Saved to:\", output_path)\n",
    "\n",
    "print(\"\\nNew pay statistics:\")\n",
    "print(df[\"pay_received\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34d360f2-474d-4cae-852b-2b2b9783485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SHAP-guided RL feature selection...\n",
      "\n",
      "Episode 00 | Reward=-66.3749 | Features=4\n",
      "Episode 01 | Reward=-63.7072 | Features=4\n",
      "Episode 02 | Reward=-66.3749 | Features=4\n",
      "Episode 03 | Reward=-63.3595 | Features=5\n",
      "Episode 04 | Reward=-63.0385 | Features=7\n",
      "Episode 05 | Reward=-61.7837 | Features=5\n",
      "Episode 06 | Reward=-63.3765 | Features=5\n",
      "Episode 07 | Reward=-66.5399 | Features=5\n",
      "Episode 08 | Reward=-84.2316 | Features=6\n",
      "Episode 09 | Reward=-99.5228 | Features=5\n",
      "Episode 10 | Reward=-61.7890 | Features=4\n",
      "Episode 11 | Reward=-66.5399 | Features=5\n",
      "Episode 12 | Reward=-61.7890 | Features=4\n",
      "Episode 13 | Reward=-65.6892 | Features=6\n",
      "Episode 14 | Reward=-1000000.0000 | Features=3\n",
      "Episode 15 | Reward=-64.4641 | Features=6\n",
      "Episode 16 | Reward=-1000000.0000 | Features=3\n",
      "Episode 17 | Reward=-1000000.0000 | Features=3\n",
      "Episode 18 | Reward=-1000000.0000 | Features=3\n",
      "Episode 19 | Reward=-65.7929 | Features=7\n",
      "Episode 20 | Reward=-61.7890 | Features=4\n",
      "Episode 21 | Reward=-74.6666 | Features=7\n",
      "Episode 22 | Reward=-65.1826 | Features=5\n",
      "Episode 23 | Reward=-65.6892 | Features=6\n",
      "Episode 24 | Reward=-66.5399 | Features=5\n",
      "Episode 25 | Reward=-1000000.0000 | Features=3\n",
      "Episode 26 | Reward=-61.7890 | Features=4\n",
      "Episode 27 | Reward=-61.7890 | Features=4\n",
      "Episode 28 | Reward=-1000000.0000 | Features=3\n",
      "Episode 29 | Reward=-66.5399 | Features=5\n",
      "\n",
      "âœ… FINAL SELECTED FEATURES:\n",
      "- is_peak_hour\n",
      "- traffic_severity\n",
      "- surge_bonus\n",
      "- effort_bonus\n",
      "- rating_penalty\n",
      "\n",
      "ðŸ“ Saved RL-selected dataset to: /Users/prajitbaskaran/Downloads/RL_2__1finalig_v02.csv\n",
      "Final shape: (45555, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "INPUT_PATH = \"/Users/prajitbaskaran/Downloads/1finalig_v02.csv\"\n",
    "OUTPUT_PATH = \"/Users/prajitbaskaran/Downloads/RL_2__1finalig_v02.csv\"\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "N_EPISODES = 30        # RL iterations\n",
    "MAX_FLIPS = 2          # PPO-style constrained action\n",
    "SUBSAMPLE_FRAC = 0.25  # speed-up\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "X_full = df.drop(columns=[TARGET])\n",
    "y_full = df[TARGET]\n",
    "\n",
    "feature_names = X_full.columns.tolist()\n",
    "N_FEATURES = len(feature_names)\n",
    "\n",
    "# Subsample for RL evaluation\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X_full, y_full, train_size=SUBSAMPLE_FRAC, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# ENVIRONMENT (FAST)\n",
    "# ===============================\n",
    "def evaluate_subset(mask):\n",
    "    selected = [f for f, m in zip(feature_names, mask) if m == 1]\n",
    "\n",
    "    if len(selected) < 4:\n",
    "        return -1e6\n",
    "\n",
    "    X_sel = X_sub[selected]\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_sel, y_sub, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=80,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, preds))\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X_tr, check_additivity=False)\n",
    "\n",
    "    shap_var = np.mean(np.var(np.abs(shap_vals), axis=0))\n",
    "\n",
    "    reward = (\n",
    "        - rmse               # accuracy\n",
    "        - 0.4 * shap_var     # explanation stability\n",
    "        - 0.03 * len(selected)  # sparsity\n",
    "    )\n",
    "\n",
    "    return reward\n",
    "\n",
    "# ===============================\n",
    "# PPO-INSPIRED RL LOOP\n",
    "# ===============================\n",
    "current_mask = np.random.randint(0, 2, N_FEATURES)\n",
    "best_mask = current_mask.copy()\n",
    "best_reward = -np.inf\n",
    "\n",
    "print(\"Starting SHAP-guided RL feature selection...\\n\")\n",
    "\n",
    "for ep in range(N_EPISODES):\n",
    "\n",
    "    candidate = current_mask.copy()\n",
    "    flip_idx = np.random.choice(\n",
    "        N_FEATURES,\n",
    "        size=np.random.randint(1, MAX_FLIPS + 1),\n",
    "        replace=False\n",
    "    )\n",
    "    candidate[flip_idx] = 1 - candidate[flip_idx]\n",
    "\n",
    "    reward = evaluate_subset(candidate)\n",
    "\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_mask = candidate.copy()\n",
    "        current_mask = candidate.copy()\n",
    "\n",
    "    print(\n",
    "        f\"Episode {ep:02d} | \"\n",
    "        f\"Reward={reward:.4f} | \"\n",
    "        f\"Features={candidate.sum()}\"\n",
    "    )\n",
    "\n",
    "# ===============================\n",
    "# FINAL FEATURE SET\n",
    "# ===============================\n",
    "selected_features = [\n",
    "    f for f, m in zip(feature_names, best_mask) if m == 1\n",
    "]\n",
    "\n",
    "print(\"\\nâœ… FINAL SELECTED FEATURES:\")\n",
    "for f in selected_features:\n",
    "    print(\"-\", f)\n",
    "\n",
    "# ===============================\n",
    "# SAVE FEATURE-SELECTED DATASET\n",
    "# ===============================\n",
    "final_df = df[selected_features + [TARGET]]\n",
    "final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\nðŸ“ Saved RL-selected dataset to:\", OUTPUT_PATH)\n",
    "print(\"Final shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0cb95c-5935-45e6-b01e-094dabb28a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random Forest RÂ² score: 0.0004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATASET\n",
    "# ===============================\n",
    "df = pd.read_csv(\"/Users/prajitbaskaran/Downloads/RL_2__1finalig_v02.csv\")\n",
    "\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "# ===============================\n",
    "# TRAIN-TEST SPLIT\n",
    "# ===============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# TRAIN RANDOM FOREST (SIMPLE)\n",
    "# ===============================\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# EVALUATE\n",
    "# ===============================\n",
    "y_pred = rf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"âœ… Random Forest RÂ² score:\", round(r2, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31525845-dd54-405b-b0e6-335bc8b665bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running constrained RL feature selection...\n",
      "\n",
      "Run 1/5 completed\n",
      "Run 2/5 completed\n",
      "Run 3/5 completed\n",
      "Run 4/5 completed\n",
      "Run 5/5 completed\n",
      "\n",
      "âœ… STABLE SELECTED FEATURES:\n",
      "- Delivery_person_Age\n",
      "- Delivery_person_Ratings\n",
      "- Vehicle_condition\n",
      "- distance_km\n",
      "- hour\n",
      "- is_peak_hour\n",
      "- weather_severity\n",
      "- traffic_severity\n",
      "- surge_bonus\n",
      "- effort_bonus\n",
      "- rating_penalty\n",
      "- policy_multiplier\n",
      "\n",
      "ðŸ“ Saved stable RL-selected dataset to: /Users/prajitbaskaran/Downloads/RLV1_1finalig_v02.csv\n",
      "Final shape: (45555, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "INPUT_PATH = \"/Users/prajitbaskaran/Downloads/1finalig_v02.csv\"\n",
    "OUTPUT_PATH = \"/Users/prajitbaskaran/Downloads/RLV1_1finalig_v02.csv\"\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "MANDATORY_FEATURES = [\"distance_km\", \"hour\"]  # hard constraints\n",
    "\n",
    "K_RUNS = 5\n",
    "SELECTION_THRESHOLD = 0.6   # 60% of runs\n",
    "N_EPISODES = 25\n",
    "MAX_FLIPS = 2\n",
    "SUBSAMPLE_FRAC = 0.25\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "X_full = df.drop(columns=[TARGET])\n",
    "y_full = df[TARGET]\n",
    "\n",
    "feature_names = X_full.columns.tolist()\n",
    "N_FEATURES = len(feature_names)\n",
    "\n",
    "feature_index = {f: i for i, f in enumerate(feature_names)}\n",
    "\n",
    "# ===============================\n",
    "# SUBSAMPLE FOR SPEED\n",
    "# ===============================\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X_full, y_full, train_size=SUBSAMPLE_FRAC, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# ENVIRONMENT FUNCTION\n",
    "# ===============================\n",
    "def evaluate_subset(mask):\n",
    "    selected = [f for f, m in zip(feature_names, mask) if m == 1]\n",
    "\n",
    "    if len(selected) < 4:\n",
    "        return -1e6\n",
    "\n",
    "    X_sel = X_sub[selected]\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_sel, y_sub, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=80,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, preds))\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X_tr, check_additivity=False)\n",
    "\n",
    "    shap_var = np.mean(np.var(np.abs(shap_vals), axis=0))\n",
    "\n",
    "    reward = (\n",
    "        - rmse\n",
    "        - 0.4 * shap_var\n",
    "        - 0.02 * len(selected)   # weaker sparsity penalty\n",
    "    )\n",
    "\n",
    "    return reward\n",
    "\n",
    "# ===============================\n",
    "# RUN RL K TIMES\n",
    "# ===============================\n",
    "all_selected_masks = []\n",
    "\n",
    "print(\"Running constrained RL feature selection...\\n\")\n",
    "\n",
    "for run in range(K_RUNS):\n",
    "    np.random.seed(42 + run)\n",
    "\n",
    "    # initialize random mask\n",
    "    current_mask = np.random.randint(0, 2, N_FEATURES)\n",
    "\n",
    "    # enforce mandatory features\n",
    "    for f in MANDATORY_FEATURES:\n",
    "        if f in feature_index:\n",
    "            current_mask[feature_index[f]] = 1\n",
    "\n",
    "    best_mask = current_mask.copy()\n",
    "    best_reward = -np.inf\n",
    "\n",
    "    for ep in range(N_EPISODES):\n",
    "        candidate = current_mask.copy()\n",
    "\n",
    "        flip_idx = np.random.choice(\n",
    "            N_FEATURES,\n",
    "            size=np.random.randint(1, MAX_FLIPS + 1),\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        candidate[flip_idx] = 1 - candidate[flip_idx]\n",
    "\n",
    "        # re-enforce mandatory features\n",
    "        for f in MANDATORY_FEATURES:\n",
    "            if f in feature_index:\n",
    "                candidate[feature_index[f]] = 1\n",
    "\n",
    "        reward = evaluate_subset(candidate)\n",
    "\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_mask = candidate.copy()\n",
    "            current_mask = candidate.copy()\n",
    "\n",
    "    all_selected_masks.append(best_mask)\n",
    "    print(f\"Run {run+1}/{K_RUNS} completed\")\n",
    "\n",
    "# ===============================\n",
    "# AGGREGATE SELECTIONS\n",
    "# ===============================\n",
    "selection_counts = np.sum(all_selected_masks, axis=0) / K_RUNS\n",
    "\n",
    "stable_features = [\n",
    "    f for f, freq in zip(feature_names, selection_counts)\n",
    "    if freq >= SELECTION_THRESHOLD\n",
    "]\n",
    "\n",
    "# ensure mandatory features are present\n",
    "for f in MANDATORY_FEATURES:\n",
    "    if f not in stable_features and f in feature_names:\n",
    "        stable_features.append(f)\n",
    "\n",
    "print(\"\\nâœ… STABLE SELECTED FEATURES:\")\n",
    "for f in stable_features:\n",
    "    print(\"-\", f)\n",
    "\n",
    "# ===============================\n",
    "# SAVE FINAL DATASET\n",
    "# ===============================\n",
    "final_df = df[stable_features + [TARGET]]\n",
    "final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\nðŸ“ Saved stable RL-selected dataset to:\", OUTPUT_PATH)\n",
    "print(\"Final shape:\", final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6d2489e-fe90-44c8-bff3-df4497517610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Saved model not found â€” training quickly\n",
      "\n",
      "ðŸ§¾ PAY EXPLANATION (GLASS BOX)\n",
      "--------------------------------------------------\n",
      "Predicted Pay : â‚¹77.58\n",
      "Actual Pay    : â‚¹80.19\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted Pay : â‚¹\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_pay\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mActual Pay    : â‚¹\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_pay\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBase Pay      : â‚¹\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_value\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ” How each factor affected your pay:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature, value \u001b[38;5;129;01min\u001b[39;00m contributions:\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ===============================\n",
    "# LOAD DATA\n",
    "# ===============================\n",
    "df = pd.read_csv(\"/Users/prajitbaskaran/Downloads/RLV1_1finalig_v02.csv\")\n",
    "TARGET = \"pay_received\"\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "# ===============================\n",
    "# LOAD OR TRAIN MODEL\n",
    "# ===============================\n",
    "try:\n",
    "    model = joblib.load(\"final_pay_model.pkl\")\n",
    "    explainer = joblib.load(\"final_shap_explainer.pkl\")\n",
    "    print(\"âœ… Loaded saved model & explainer\")\n",
    "except:\n",
    "    print(\"âš ï¸ Saved model not found â€” training quickly\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# ===============================\n",
    "# PICK ONE DELIVERY TO EXPLAIN\n",
    "# ===============================\n",
    "idx = 0  # change this to explain any delivery\n",
    "\n",
    "x_instance = X.iloc[[idx]]\n",
    "actual_pay = y.iloc[idx]\n",
    "predicted_pay = model.predict(x_instance)[0]\n",
    "\n",
    "# ===============================\n",
    "# COMPUTE SHAP VALUES\n",
    "# ===============================\n",
    "shap_values = explainer.shap_values(x_instance)[0]\n",
    "base_value = explainer.expected_value\n",
    "\n",
    "# ===============================\n",
    "# BUILD PLAIN-ENGLISH EXPLANATION\n",
    "# ===============================\n",
    "contributions = list(zip(X.columns, shap_values))\n",
    "contributions = sorted(contributions, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nðŸ§¾ PAY EXPLANATION (GLASS BOX)\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(f\"Predicted Pay : â‚¹{predicted_pay:.2f}\")\n",
    "print(f\"Actual Pay    : â‚¹{actual_pay:.2f}\")\n",
    "print(f\"Base Pay      : â‚¹{base_value:.2f}\\n\")\n",
    "\n",
    "print(\"ðŸ” How each factor affected your pay:\\n\")\n",
    "\n",
    "for feature, value in contributions:\n",
    "    if abs(value) < 0.5:\n",
    "        continue  # ignore tiny effects\n",
    "\n",
    "    if value > 0:\n",
    "        print(f\"âž• {feature} increased your pay by â‚¹{value:.2f}\")\n",
    "    else:\n",
    "        print(f\"âž– {feature} reduced your pay by â‚¹{abs(value):.2f}\")\n",
    "\n",
    "print(\"\\nðŸ§® Final Explanation:\")\n",
    "print(\n",
    "    f\"Starting from a base pay of â‚¹{base_value:.2f}, \"\n",
    "    f\"the above factors were added or subtracted to arrive at \"\n",
    "    f\"a final predicted pay of â‚¹{predicted_pay:.2f}.\"\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# OPTIONAL: VISUAL FORCE PLOT\n",
    "# ===============================\n",
    "shap.force_plot(\n",
    "    base_value,\n",
    "    shap_values,\n",
    "    x_instance,\n",
    "    matplotlib=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ca7e9-f0d9-481b-a7d8-6f03a1bc8ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
